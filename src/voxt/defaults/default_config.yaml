# app_mode: whisp
log_enabled: true
log_location: ''
perf_collect: false
perf_accuracy_rating_collect: true
simulate_typing: true
typing_delay: 1
typing_start_delay: 0.15
ctrl_v_paste: false  # Use Ctrl+V instead of default Ctrl+Shift+V
verbosity: true
whisper_binary: whisper.cpp/build/bin/whisper-cli
model_path: whisper.cpp/models/ggml-base.en.bin

# --- âœ¨ AIPP (AI post-processing) ------------------------------------------
aipp_enabled: false
aipp_provider: ollama
aipp_active_prompt: default

# New: List of models per provider
aipp_models:
  ollama: ["llama3.2:latest", "mistral:latest", "gemma3:latest", "qwen2.5-coder:1.5b"]
  openai: ["gpt-4o-mini-2024-07-18"]
  anthropic: ["claude-3-opus-20240229", "claude-3-haiku"]
  xai: ["grok-3-latest"]
  llamacpp_server: ["qwen2.5-3b-instruct-q4_k_m"]
  llamacpp_direct: ["qwen2.5-3b-instruct-q4_k_m"]

# New: Selected model per provider
aipp_selected_models:
  ollama: "llama3.2:latest"
  openai: "gpt-4o-mini-2024-07-18"
  anthropic: "claude-3-opus-20240229"
  xai: "grok-3-latest"
  llamacpp_server: "qwen2.5-3b-instruct-q4_k_m"
  llamacpp_direct: "qwen2.5-3b-instruct-q4_k_m"

# llama.cpp settings
llamacpp_server_path: "llama.cpp/build/bin/llama-server"
llamacpp_cli_path: "llama.cpp/build/bin/llama-cli"
llamacpp_default_model: "llamacpp_models/qwen2.5-3b-instruct-q4_k_m.gguf"
llamacpp_server_url: "http://localhost:8080"
llamacpp_server_timeout: 30n



aipp_prompts:
  default: "Rewrite the following input so that it is clean and concise. Do not add any additional text or commentary. Just the rewritten text."
  prompt1: ""
  prompt2: ""
  prompt3: ""